{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "r2d2_hw5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QgSA_YXpHwFI",
        "blzu6p9hoJgi",
        "3Hh5RkEiHrt8",
        "x8v6GBKws0Z7",
        "Y9dazGvsOzs7",
        "RH1nb2_NrEv0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9wp38iEgQJa"
      },
      "source": [
        "**In this homework, you will implement several AI models to conduct the intent detection task.**\n",
        "![alt text](https://i.ibb.co/fXmYHRq/ec5.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgSA_YXpHwFI"
      },
      "source": [
        "# Part 0: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Acquisition"
      ],
      "metadata": {
        "id": "KN8vQjds4eTW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuPUTB25ghjz"
      },
      "source": [
        "In this section, you will have a general idea of how the data looks like and do some simple transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKbddkNKfmNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf9c319-24df-4893-caf3-2a548aa14fcc"
      },
      "source": [
        "# download the data\n",
        "!wget \"https://drive.google.com/uc?export=download&id=1dLUN9oSB4u27NOleYE-Uksoh6RNQlZbi\" -O sample.p"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-09 06:31:45--  https://drive.google.com/uc?export=download&id=1dLUN9oSB4u27NOleYE-Uksoh6RNQlZbi\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.157.138, 142.250.157.101, 142.250.157.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.157.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lgsenkogkf9lriku3pi5gr8ihg3f7dd7/1639031475000/15787019596848476183/*/1dLUN9oSB4u27NOleYE-Uksoh6RNQlZbi?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-12-09 06:31:46--  https://doc-10-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lgsenkogkf9lriku3pi5gr8ihg3f7dd7/1639031475000/15787019596848476183/*/1dLUN9oSB4u27NOleYE-Uksoh6RNQlZbi?e=download\n",
            "Resolving doc-10-a0-docs.googleusercontent.com (doc-10-a0-docs.googleusercontent.com)... 142.251.8.132, 2404:6800:4008:c15::84\n",
            "Connecting to doc-10-a0-docs.googleusercontent.com (doc-10-a0-docs.googleusercontent.com)|142.251.8.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 285436 (279K) [text/x-pascal]\n",
            "Saving to: ‘sample.p’\n",
            "\n",
            "sample.p            100%[===================>] 278.75K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-12-09 06:31:46 (128 MB/s) - ‘sample.p’ saved [285436/285436]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4at6WgHmCra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0bcd260-cdd8-4507-bab2-ac82e62796b7"
      },
      "source": [
        "# test sentences for evaluation\n",
        "!wget \"https://drive.google.com/uc?export=download&id=1gEW_qY5x8uPAhriiobubheYo6FC35btQ\" -O test_sentences.p"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-09 06:31:46--  https://drive.google.com/uc?export=download&id=1gEW_qY5x8uPAhriiobubheYo6FC35btQ\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.157.139, 142.250.157.101, 142.250.157.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.157.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/evdl9qlq1glqndks0mvckjh21uhdqrlv/1639031475000/15787019596848476183/*/1gEW_qY5x8uPAhriiobubheYo6FC35btQ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-12-09 06:31:46--  https://doc-0o-a0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/evdl9qlq1glqndks0mvckjh21uhdqrlv/1639031475000/15787019596848476183/*/1gEW_qY5x8uPAhriiobubheYo6FC35btQ?e=download\n",
            "Resolving doc-0o-a0-docs.googleusercontent.com (doc-0o-a0-docs.googleusercontent.com)... 142.251.8.132, 2404:6800:4008:c15::84\n",
            "Connecting to doc-0o-a0-docs.googleusercontent.com (doc-0o-a0-docs.googleusercontent.com)|142.251.8.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52258 (51K) [text/x-pascal]\n",
            "Saving to: ‘test_sentences.p’\n",
            "\n",
            "test_sentences.p    100%[===================>]  51.03K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-12-09 06:31:47 (82.9 MB/s) - ‘test_sentences.p’ saved [52258/52258]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYqcSyQsmgf1"
      },
      "source": [
        "import pickle\n",
        "samples = pickle.load(open(\"sample.p\", \"rb\"))\n",
        "test_sentences = pickle.load(open(\"test_sentences.p\", \"rb\"))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AicwRkV-mzqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7883ebb5-5350-4536-f0b8-09c3b4fc281f"
      },
      "source": [
        "###data structure###\n",
        "### [[sentence, label]] ###\n",
        "print(samples[:3])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Turn off the holoemitter.', 2], ['Halt.', 1], ['Get off tiptoes', 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1m_-5m2eymC"
      },
      "source": [
        "There are nine categories for these sentences, which are 'no', 'driving', 'light', 'head', 'state', 'connection', 'stance', 'animation' and 'grid'. The mapping from index to category name are shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YapxZx0sWhDg"
      },
      "source": [
        "ind2cat = {0: 'no', 1: 'driving', 2: 'light', 3: 'head', 4: 'state', 5: 'connection', 6: 'stance', 7: 'animation', 8: 'grid'}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1hHO6pInCH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d2bdfc-5294-42cb-9e34-2be1ad4bc818"
      },
      "source": [
        "### Distribution on categories ###\n",
        "cat2sentence = {}\n",
        "for sample in samples:\n",
        "  sentence = sample[0]\n",
        "  cat = ind2cat[sample[1]]\n",
        "  if cat not in cat2sentence:\n",
        "    cat2sentence[cat] = [sentence]\n",
        "  else:\n",
        "    cat2sentence[cat].append(sentence)\n",
        "\n",
        "print(\"number of sentences for each category\")\n",
        "for cat, sentences in cat2sentence.items():\n",
        "  print(cat, \": \", len(sentences))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of sentences for each category\n",
            "light :  716\n",
            "driving :  784\n",
            "stance :  758\n",
            "head :  698\n",
            "grid :  678\n",
            "state :  676\n",
            "animation :  645\n",
            "no :  629\n",
            "connection :  673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blzu6p9hoJgi"
      },
      "source": [
        "### Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReEMaskjoMZt"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "SENTENCES = [sample[0] for sample in samples]\n",
        "LABELS = [sample[1] for sample in samples]\n",
        "X_train, X_val, y_train, y_val = train_test_split(SENTENCES, LABELS, test_size=0.2, random_state=7)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hh5RkEiHrt8"
      },
      "source": [
        "### Clean Text\n",
        "Write a tokenization function clean(sentence) which takes as input a string of text and returns a list of tokens derived from that text. Here, we define a token to be a contiguous sequence of non-whitespace characters. We will remove punctuation marks and convert the text to lowercase. Hint: Use the built-in constant string.punctuation, found in the string module, and/or python's regex library, re."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM0rg6vdHmxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e852e82f-198d-499a-e74c-2c7bbc1d1d71"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = stopwords.words('english')\n",
        "\n",
        "def clean(sentence):\n",
        "    ''' 1. tokenize the sentence (remove punctuation)\n",
        "        2. remove the stop words\n",
        "        3. convert all words to lowercase\n",
        "    '''\n",
        "    sentence = sentence.lower()\n",
        "    no_punct = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "    tokens = no_punct.split(' ')\n",
        "\n",
        "    stop = set(STOPWORDS)\n",
        "    sentence = [token for token in tokens if token not in stop]\n",
        "    \n",
        "    return sentence;\n",
        "#pass\n",
        "\n",
        "X_train_token = [clean(sentence) for sentence in X_train]\n",
        "X_val_token = [clean(sentence) for sentence in X_val]\n",
        "X_train_val = X_train_token + X_val_token"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRb6Z7qBNgsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b03333-ddb3-4fec-a2a7-8d61954b9a4c"
      },
      "source": [
        "max_len = 0# Find the maximum length of tokens in train/val\n",
        "for tokenized in X_train_token:\n",
        "    l = len(tokenized)\n",
        "    if l > max_len:\n",
        "        max_len = l\n",
        "for tokenized in X_val_token:\n",
        "    l = len(tokenized)\n",
        "    if l > max_len:\n",
        "        max_len = l\n",
        "\n",
        "print('The maximum length of tokens in our dataset is: ', max_len, ' tokens')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum length of tokens in our dataset is:  31  tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CskkruUAMMfI"
      },
      "source": [
        "### Build a Vocabulary\n",
        "Build a vocabulary to map each word to an index, you need to first find the unique words in train/val set.\n",
        "\n",
        "Once you build a vocabulary, it's better to save it to a file for future use. Because the vocabulary may change each time you run the code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "temp = [element for sample in X_train_val for element in sample]\n",
        "\n",
        "X_train_val_counts = Counter(temp)\n",
        "counts = Counter(X_train_val_counts)"
      ],
      "metadata": {
        "id": "juNwKKt0kgqv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAxoqREjMMCI"
      },
      "source": [
        "word_count = dict(counts) # count the frequency of each word\n",
        "word2ind = {} # build your vocabulary\n",
        "\n",
        "words = list(word_count.keys())\n",
        "for i, w in enumerate(words):\n",
        "    word2ind[w] = i+1\n",
        "\n",
        "vocab_size = len(word2ind)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuybRo1HqO-"
      },
      "source": [
        "# Part 1: Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow.random import set_seed\n",
        "set_seed(2)"
      ],
      "metadata": {
        "id": "GO5Xbv7PugDT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opl1oMHFN0Ph"
      },
      "source": [
        "### Convert token to vector\n",
        "Convert each list of tokens into an array use the vocabulary you built before. The length of the vector is the max_len and remember to do zero-padding if a list's lenghth is smaller than max_len."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5zwLf-SOU0W"
      },
      "source": [
        "def vectorize(tokens, max_len, word2ind):\n",
        "    '''\n",
        "        Input: list of tokens\n",
        "        Output: 1D numpy array (length = max_len)\n",
        "    '''\n",
        "    inds = []\n",
        "    for token in tokens:\n",
        "        if token in word2ind.keys():\n",
        "            ind = word2ind[token]\n",
        "        else:\n",
        "            ind = 0\n",
        "        inds.append(ind)\n",
        "    n = len(inds)\n",
        "    if n < max_len:\n",
        "        zero_pad = [0 for i in range(max_len - n)]\n",
        "        inds.extend(zero_pad)\n",
        "    elif n > max_len:\n",
        "        inds = inds[:max_len]\n",
        "    return np.array(inds);\n",
        "#pass\n",
        "\n",
        "X_train_array = np.array([vectorize(tokens, max_len, word2ind) for tokens in X_train_token])\n",
        "X_val_array = np.array([vectorize(tokens, max_len, word2ind) for tokens in X_val_token])\n",
        "assert X_train_array.shape[-1] == max_len"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATfeiT7IUftX"
      },
      "source": [
        "### One-hot label\n",
        "Convert the scalar label to 1D array (length = 9), e.g 0 -> array([1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gru9TAqnUfYh"
      },
      "source": [
        "y_train_onehot = []\n",
        "for label in y_train:\n",
        "    zeros = [0 for i in range(9)]\n",
        "    zeros[label] = 1\n",
        "    y_train_onehot.append(zeros)\n",
        "y_train_onehot = np.array(y_train_onehot)\n",
        "\n",
        "y_val_onehot = []\n",
        "for label in y_val:\n",
        "    zeros = [0 for i in range(9)]\n",
        "    zeros[label] = 1\n",
        "    y_val_onehot.append(zeros)\n",
        "y_val_onehot = np.array(y_val_onehot)\n",
        "\n",
        "assert y_train_onehot.shape[1] == 9"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsp0pgj6PI6A"
      },
      "source": [
        "### Build the Recurrent Neural Network\n",
        "Now it's time to build the RNN network to do the classification task, you could just refer to this [official document](https://www.tensorflow.org/guide/keras/rnn).\n",
        "\n",
        "You will need the Embedding layer, RNN layer and Dense layer, your last layer should project to the number of labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ODSzrR2RbbR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade7fe82-84a8-4711-9dab-ccc1e32fced5"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "# Embedding Layer, Input Dimension = vocab_size, Output Dimension = 64\n",
        "model.add(layers.Embedding(vocab_size, 64))\n",
        "\n",
        "# Two LSTM layers with 64 Units\n",
        "model.add(layers.LSTM(64, return_sequences=True))\n",
        "model.add(layers.LSTM(64))\n",
        "\n",
        "# Dense to the number of classes with softmax activation function\n",
        "model.add(layers.Dense(9, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          91968     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 64)          33024     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 158,601\n",
            "Trainable params: 158,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oLUl6dVYTIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12211ee4-35c7-4cbf-8f03-81ea089078a1"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(X_train_array, y_train_onehot, batch_size=8, epochs=12, validation_data=(X_val_array, y_val_onehot))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "626/626 [==============================] - 9s 9ms/step - loss: 1.9760 - accuracy: 0.1956 - val_loss: 1.7756 - val_accuracy: 0.2548\n",
            "Epoch 2/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 1.6948 - accuracy: 0.2931 - val_loss: 1.6911 - val_accuracy: 0.2891\n",
            "Epoch 3/12\n",
            "626/626 [==============================] - 5s 7ms/step - loss: 1.5782 - accuracy: 0.3305 - val_loss: 1.5484 - val_accuracy: 0.3450\n",
            "Epoch 4/12\n",
            "626/626 [==============================] - 4s 7ms/step - loss: 1.4008 - accuracy: 0.4016 - val_loss: 1.4397 - val_accuracy: 0.4113\n",
            "Epoch 5/12\n",
            "626/626 [==============================] - 4s 7ms/step - loss: 1.2870 - accuracy: 0.4865 - val_loss: 1.5081 - val_accuracy: 0.4073\n",
            "Epoch 6/12\n",
            "626/626 [==============================] - 4s 7ms/step - loss: 1.2563 - accuracy: 0.4931 - val_loss: 1.2469 - val_accuracy: 0.4968\n",
            "Epoch 7/12\n",
            "626/626 [==============================] - 4s 7ms/step - loss: 1.1462 - accuracy: 0.5686 - val_loss: 1.0624 - val_accuracy: 0.6238\n",
            "Epoch 8/12\n",
            "626/626 [==============================] - 4s 7ms/step - loss: 0.8306 - accuracy: 0.7219 - val_loss: 0.7317 - val_accuracy: 0.7564\n",
            "Epoch 9/12\n",
            "626/626 [==============================] - 5s 7ms/step - loss: 0.5911 - accuracy: 0.8064 - val_loss: 0.5941 - val_accuracy: 0.7915\n",
            "Epoch 10/12\n",
            "626/626 [==============================] - 4s 7ms/step - loss: 0.5701 - accuracy: 0.8082 - val_loss: 0.5930 - val_accuracy: 0.8203\n",
            "Epoch 11/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 0.4693 - accuracy: 0.8597 - val_loss: 0.4900 - val_accuracy: 0.8762\n",
            "Epoch 12/12\n",
            "626/626 [==============================] - 5s 7ms/step - loss: 0.3894 - accuracy: 0.8941 - val_loss: 0.4757 - val_accuracy: 0.8730\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f70c02628d0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH1Xe7EMlITl"
      },
      "source": [
        "### Evaluate on the test sentences\n",
        "Now run your model to predict on the test sentences, you need to do the preprocessing on these sentences first and save your prediction to a list of labels, e.g [0, 2, 1, 5, ....]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBKwqNUElnyu"
      },
      "source": [
        "test_prediction = []\n",
        "#TODO\n",
        "X_test_token = [clean(sentence) for sentence in test_sentences]\n",
        "\n",
        "first = True\n",
        "for tokens in X_test_token:\n",
        "    if first:\n",
        "        X_test_array = vectorize(tokens, max_len, word2ind)\n",
        "        X_test_array = X_test_array[np.newaxis, :]\n",
        "        first = False\n",
        "    else:\n",
        "        temp = vectorize(tokens, max_len, word2ind)\n",
        "        temp = temp[np.newaxis, :]\n",
        "        X_test_array = np.vstack((X_test_array, temp))\n",
        "y_preds = model.predict(X_test_array, batch_size=8)\n",
        "test_prediction = np.argmax(y_preds, axis=1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbZmj4Dolo0-"
      },
      "source": [
        "# Save the results and upload to Gradescope\n",
        "pickle.dump(test_prediction, open(\"rnn.p\", \"wb\"))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8v6GBKws0Z7"
      },
      "source": [
        "#Part 2. Word Embedding via pymagnitude\n",
        "Instead of using the vocabulary to convert word to number, you could use pretrained word embeddings to do the task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbMVzcGDucgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc514ae-9b19-4366-b301-929d24d10e1e"
      },
      "source": [
        "! echo \"Installing Magnitude.... (please wait, can take a while)\"\n",
        "! (curl https://raw.githubusercontent.com/plasticityai/magnitude/master/install-colab.sh | /bin/bash 1>/dev/null 2>/dev/null)\n",
        "! echo \"Done installing Magnitude.\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Magnitude.... (please wait, can take a while)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   137  100   137    0     0    324      0 --:--:-- --:--:-- --:--:--   324\n",
            "Done installing Magnitude.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VzKjJNNxkYH"
      },
      "source": [
        "Next, you'll need to download a pre-trained set of word embeddings. We'll get a set trained with Google's word2vec algorithm, which we discussed in class. [Here](https://gitlab.com/Plasticity/magnitude), you can check the full list of available embeddings, feel free to try different embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfDkasoHxLjD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab6d2ae-2c15-4efe-a430-502d772978dc"
      },
      "source": [
        "# Download Pretrained Word-Embedding\n",
        "! wget http://magnitude.plasticity.ai/word2vec/light/GoogleNews-vectors-negative300.magnitude"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-09 06:34:34--  http://magnitude.plasticity.ai/word2vec/light/GoogleNews-vectors-negative300.magnitude\n",
            "Resolving magnitude.plasticity.ai (magnitude.plasticity.ai)... 52.216.81.162\n",
            "Connecting to magnitude.plasticity.ai (magnitude.plasticity.ai)|52.216.81.162|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4211335168 (3.9G) [binary/octet-stream]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.magnitude.4’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   3.92G  16.7MB/s    in 5m 6s   \n",
            "\n",
            "2021-12-09 06:39:41 (13.1 MB/s) - ‘GoogleNews-vectors-negative300.magnitude.4’ saved [4211335168/4211335168]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngFFKNj8yAU5"
      },
      "source": [
        "# Load the embedding\n",
        "from pymagnitude import *\n",
        "vectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\") \n",
        "D = vectors.query(\"cat\").shape[0]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pdRsfIuVxIg"
      },
      "source": [
        "### Convert tokens to embeddings\n",
        "You could now use the pymagnitude to query each token and convert them to a list of embeddings. Note that you need to do zero padding to match the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0LMW9_OXjs-"
      },
      "source": [
        "def embedding(list_tokens, max_len, vectors, D):\n",
        "    '''\n",
        "    return an array with the shape (n_of_samples, max_len, D)\n",
        "    '''\n",
        "    embeddings = np.empty((len(list_tokens), max_len, D))\n",
        "\n",
        "    for i, tokens in enumerate(list_tokens):\n",
        "        for j, t in enumerate(tokens):\n",
        "            if j == max_len:\n",
        "                break;\n",
        "            v = vectors.query(t)\n",
        "            embeddings[i, j, :] = v\n",
        "    return embeddings;\n",
        "#pass\n",
        "X_train_embedding = embedding(X_train_token, max_len, vectors, D)\n",
        "X_val_embedding = embedding(X_val_token, max_len, vectors, D)\n",
        "\n",
        "assert X_train_embedding.shape[-1] == D\n",
        "assert X_train_embedding.shape[-2] == max_len"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxoxUCB8YPul"
      },
      "source": [
        "### Build the RNN model\n",
        "Similar to Part 1, build a RNN model using your new embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER6xPrArYPLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e577a809-cd5f-45a9-c1ab-56cb0173a4b6"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "#TODO\n",
        "# LSTM Layer with input shape (max_len, D), output shape (max_len, 256)\n",
        "model.add(layers.LSTM(256, return_sequences=True))\n",
        "\n",
        "# LSTM Layer with 128 units\n",
        "model.add(layers.LSTM(128))\n",
        "\n",
        "# Dense to 64 with tanh activation function\n",
        "model.add(layers.Dense(64, activation='tanh'))\n",
        "\n",
        "# Dense to number of classes with softmax function\n",
        "model.add(layers.Dense(9, activation='softmax'))\n",
        "\n",
        "model.build(input_shape=(8, max_len, D))\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (8, 31, 256)              570368    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (8, 128)                  197120    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (8, 64)                   8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (8, 9)                    585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 776,329\n",
            "Trainable params: 776,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDyV_5F2kwFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3327f52-0bf3-4281-81c9-f1927345494c"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(X_train_embedding, y_train_onehot, batch_size=8, epochs=12, validation_data=(X_val_embedding, y_val_onehot))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "626/626 [==============================] - 9s 9ms/step - loss: 1.5094 - accuracy: 0.3898 - val_loss: 1.2591 - val_accuracy: 0.4577\n",
            "Epoch 2/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 1.0576 - accuracy: 0.5868 - val_loss: 0.9467 - val_accuracy: 0.6382\n",
            "Epoch 3/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 0.8591 - accuracy: 0.6757 - val_loss: 0.8383 - val_accuracy: 0.6733\n",
            "Epoch 4/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 0.7253 - accuracy: 0.7542 - val_loss: 0.9485 - val_accuracy: 0.7173\n",
            "Epoch 5/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 0.5645 - accuracy: 0.8236 - val_loss: 0.5143 - val_accuracy: 0.8419\n",
            "Epoch 6/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 0.4332 - accuracy: 0.8755 - val_loss: 0.5815 - val_accuracy: 0.7923\n",
            "Epoch 7/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 0.3884 - accuracy: 0.8793 - val_loss: 0.4324 - val_accuracy: 0.8778\n",
            "Epoch 8/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 0.3120 - accuracy: 0.9075 - val_loss: 0.3820 - val_accuracy: 0.8922\n",
            "Epoch 9/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 0.2534 - accuracy: 0.9231 - val_loss: 0.3322 - val_accuracy: 0.9073\n",
            "Epoch 10/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 0.2066 - accuracy: 0.9387 - val_loss: 0.3258 - val_accuracy: 0.9050\n",
            "Epoch 11/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 0.2188 - accuracy: 0.9363 - val_loss: 0.2786 - val_accuracy: 0.9113\n",
            "Epoch 12/12\n",
            "626/626 [==============================] - 5s 8ms/step - loss: 0.1679 - accuracy: 0.9502 - val_loss: 0.2591 - val_accuracy: 0.9361\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ee2158450>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGEJ-FtZl3nN"
      },
      "source": [
        "### Evaluate on the test sentences\n",
        "Now run your model to predict on the test sentences, you need to do the preprocessing on these sentences first and save your prediction to a list of labels, e.g [0, 2, 1, 5, ....]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8g9CBNAl8Eo"
      },
      "source": [
        "test_prediction = []\n",
        "#TODO\n",
        "\n",
        "X_test_embedding = embedding(X_test_token, max_len, vectors, D)\n",
        "y_pred = model.predict(X_test_embedding, batch_size=8)\n",
        "test_predictions = np.argmax(y_pred, axis=1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAp7kXXpl_L3"
      },
      "source": [
        "# Save the results and upload to Gradescope\n",
        "pickle.dump(test_predictions, open(\"embedding.p\", \"wb\"))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9dazGvsOzs7"
      },
      "source": [
        "# Part 3: BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQZlpz9xMqFo"
      },
      "source": [
        "In this part, you will use the BERT pipeline to further improve the performance.\n",
        "\n",
        "This part is open-ended, we just provide one example of using BERT, feel free to find other tutorial online to customize on this task.\n",
        "\n",
        "[Here](https://huggingface.co/models) is the list of all existing models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuQDOJNpONp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019178fe-81cd-489a-8b00-b915167f21be"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.16.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt7_h3tkOili",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ead2277-bf38-41a7-a3a8-7cc66ac600bc"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig, TFBertForSequenceClassification\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") #feel free to change the model\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=9)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59drUhQZO7ep"
      },
      "source": [
        "### Use BERT Tokenizer to preprocess the data\n",
        "The BERT Tokenizer will return a dictionary which contains 'input_ids', 'token_type_ids' and 'attention_mask', we will use the 'input_ids' and 'attention_mask' later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HJrt4odOx-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b597337f-22ad-44fb-b4cc-6f8eb7fabcb4"
      },
      "source": [
        "# Test the tokenizer\n",
        "sent = X_train[0]\n",
        "tokenized_sequence = bert_tokenizer.encode_plus(sent,add_special_tokens = True,\n",
        "                                              max_length =30,pad_to_max_length = True, \n",
        "                                              return_attention_mask = True)\n",
        "print(tokenized_sequence)\n",
        "print(bert_tokenizer.decode(tokenized_sequence['input_ids']))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 7632, 1010, 1045, 2052, 2066, 2000, 2173, 2019, 2344, 2005, 2796, 2833, 2005, 2202, 5833, 2005, 2093, 2111, 1010, 3531, 1012, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}\n",
            "[CLS] hi, i would like to place an order for indian food for takeout for three people, please. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y1cnxanRD1d"
      },
      "source": [
        "Use the bert tokenizer described above, encode the training and validations sentences, note that the max length should be 64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpBf_0Z_P4oz"
      },
      "source": [
        "def BERT_Tokenizer(sentences):\n",
        "    '''Input: list of sentences\n",
        "        Output: two numpy array\n",
        "    '''\n",
        "    max_len = 64\n",
        "    input_ids_arr = np.empty((len(sentences), max_len), dtype=np.int64)\n",
        "    attention_mask_arr = np.empty((len(sentences), max_len), dtype=np.int64)\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        tokenized_sequence = bert_tokenizer.encode_plus(sentence, add_special_tokens=True, truncation=True,\n",
        "                                                max_length=max_len, padding='max_length', \n",
        "                                                return_attention_mask=True)\n",
        "        input_ids = tokenized_sequence['input_ids']\n",
        "        attention_mask = tokenized_sequence['attention_mask']\n",
        "        input_ids_arr[i, :] = input_ids\n",
        "        attention_mask_arr[i, :] = attention_mask\n",
        "    return input_ids_arr, attention_mask_arr;\n",
        "#pass\n",
        "\n",
        "X_train_ids, X_train_masks = BERT_Tokenizer(X_train)\n",
        "X_val_ids, X_val_masks = BERT_Tokenizer(X_val)\n",
        "y_train_array = np.array(y_train)\n",
        "y_val_array = np.array(y_val)\n",
        "assert X_train_ids.shape[-1] == 64"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MypXAdRKR0Cp"
      },
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6,epsilon=1e-08)\n",
        "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXA2UDc3SaAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a355337c-dc63-44f3-d2e2-57042a5d6a58"
      },
      "source": [
        "bert_model.fit([X_train_ids,X_train_masks],y_train_array,batch_size=16,epochs=6,validation_data=([X_val_ids,X_val_masks],y_val_array))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "313/313 [==============================] - 73s 173ms/step - loss: 1.5545 - accuracy: 0.5978 - val_loss: 0.8642 - val_accuracy: 0.8866\n",
            "Epoch 2/6\n",
            "313/313 [==============================] - 52s 166ms/step - loss: 0.6005 - accuracy: 0.9099 - val_loss: 0.3229 - val_accuracy: 0.9353\n",
            "Epoch 3/6\n",
            "313/313 [==============================] - 52s 165ms/step - loss: 0.2763 - accuracy: 0.9499 - val_loss: 0.1770 - val_accuracy: 0.9617\n",
            "Epoch 4/6\n",
            "313/313 [==============================] - 52s 165ms/step - loss: 0.1680 - accuracy: 0.9680 - val_loss: 0.1297 - val_accuracy: 0.9736\n",
            "Epoch 5/6\n",
            "313/313 [==============================] - 52s 165ms/step - loss: 0.1185 - accuracy: 0.9766 - val_loss: 0.1037 - val_accuracy: 0.9800\n",
            "Epoch 6/6\n",
            "313/313 [==============================] - 52s 165ms/step - loss: 0.0881 - accuracy: 0.9814 - val_loss: 0.0853 - val_accuracy: 0.9808\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6de7de7c90>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpWa8cyTXfV"
      },
      "source": [
        "### Evaluate on test sentences\n",
        "Again, use BERT to predict on the test sentences and submit to Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkNQDrOYTkf1"
      },
      "source": [
        "#TODO\n",
        "X_test_ids, X_test_masks = BERT_Tokenizer(test_sentences)\n",
        "y_pred = bert_model.predict([X_test_ids, X_test_masks], batch_size=8)\n",
        "test_predictions = np.argmax(y_pred['logits'], axis=1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3MItfdrTp8C"
      },
      "source": [
        "pickle.dump(test_predictions, open(\"bert.p\", \"wb\"))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(test_predictions, open(\"best.p\", \"wb\"))"
      ],
      "metadata": {
        "id": "c05laCi43aeA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH1nb2_NrEv0"
      },
      "source": [
        "# Part 4: Write your own commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK3_t0qzrNqs"
      },
      "source": [
        "Please write 10 sentences for each category, this will be very helpful for future students!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_no_sentences = []\n",
        "my_driving_sentences = ['Drive Forward for 2 seconds and then come beck.',\n",
        "                        'Drive in a small square.',\n",
        "                        'Stop!',\n",
        "                        'Drive in a serpentine pattern.',\n",
        "                        'Drive North for 3 seconds, then stop.',\n",
        "                        'I dont want to talk to you right now.',\n",
        "                        'Pick up the pace!',\n",
        "                        'Go for a joyride.',\n",
        "                        'Tokyo Drifting!',\n",
        "                        'Make a boston left turn.']\n",
        "\n",
        "my_light_sentences = ['Sir, did you know that your tail light is out?',\n",
        "                      'The lights are too bright!',\n",
        "                      'I cant see anything!',\n",
        "                      'Turn of the holoemitter.',\n",
        "                      'Turn on party mode.',\n",
        "                      'Turn your front light green and your tail light blue.',\n",
        "                      'Maximum Brightness.',\n",
        "                      'The room could use some color.',\n",
        "                      'Let it shine!',\n",
        "                      'Kill all the lights.']\n",
        "\n",
        "my_head_sentences = ['Turn and face me like a man!',\n",
        "                     'Face the corner and think about what you have done.',\n",
        "                     'Turn your head like the girl from The Exorcist.',\n",
        "                     'Look over there!',\n",
        "                     'Turn your head 90 degrees.',\n",
        "                     'Look around, its beautiful out!',\n",
        "                     'Shake your head.',\n",
        "                     'Turn on surveillance mode', # rotate head like a satelite dish\n",
        "                     'Act like you are malfunctioning.' # quickly rotate head back an forth\n",
        "                     'Look away.']\n",
        "\n",
        "my_state_sentences = ['Is there a ledge in front of you?',\n",
        "                      'What are the colors of each of your lights?',\n",
        "                      'Tell me your current heading',\n",
        "                      'Tell me where your head is at.',\n",
        "                      'What is your battery level?',\n",
        "                      'What stance are you in?',\n",
        "                      'Tell me if there are any AprilTags in your camera view.',\n",
        "                      'Give me your accelerometer data.',\n",
        "                      'How fast are you moving right now?',\n",
        "                      'How long have you been driving in this direction.']\n",
        "\n",
        "my_connection_sentences = ['DC.',\n",
        "                           'Disconnect from the server.',\n",
        "                           'Were being hacked, abort, abort.',\n",
        "                           'Connect to the nearest server.',\n",
        "                           'Give me your IP address.',\n",
        "                           'Start a Camera Server.',\n",
        "                           'Start a Ultrasonic Server.',\n",
        "                           'Start servers for all sensors.',\n",
        "                           'Disconnect from all servers.',\n",
        "                           'Stay connected to the main server, but disconnect from sensor servers.']\n",
        "\n",
        "my_stance_sentences = ['Assume attack position.',\n",
        "                       'Start waddling.',\n",
        "                       'Stop waddling.',\n",
        "                       'Assume resting position.',\n",
        "                       'Hey R2D2.',\n",
        "                       'Get ready to move.',\n",
        "                       'Ok R2.',\n",
        "                       'Act excited.',\n",
        "                       'Stand up straight.',\n",
        "                       'Use the force to move your wheel down.']\n",
        "\n",
        "my_animation_sentences = ['Get Hype!',\n",
        "                          'Act sad.',\n",
        "                          'Act happy.',\n",
        "                          'Do your happy dance.',\n",
        "                          'Do some donuts to celebrate.',\n",
        "                          'Do the Robot Dance.',\n",
        "                          'Can you come sit next to me?',\n",
        "                          'Pretend to be scared.',\n",
        "                          'Tip yourself over.',\n",
        "                          'Act like a turtle on its back.']\n",
        "\n",
        "my_grid_sentences = ['The square in front of you has an obstacle in it.',\n",
        "                     'Move to the square to your left.',\n",
        "                     'Find the shorteset path between the bottom left square and the top right square.',\n",
        "                     'The grid has 10x10 gridcells.',\n",
        "                     'Are there any paths between the current cell and the bottom right cell?',\n",
        "                     'What is the fastest way from point A to point B?',\n",
        "                     'How many gridcells have you visited so far?',\n",
        "                     'Visit every node once.',\n",
        "                     'How many gridcells left in the current path?',\n",
        "                     'Go to the left of the nearest obstacle.']"
      ],
      "metadata": {
        "id": "ghocjuHBaq9R"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b71YkD41rMrc"
      },
      "source": [
        "my_commands = {'no': my_no_sentences, \n",
        "               'driving': my_driving_sentences, \n",
        "               'light': my_light_sentences,\n",
        "               'head': my_head_sentences,\n",
        "               'state': my_state_sentences,\n",
        "               'connection': my_connection_sentences, \n",
        "               'stance': my_stance_sentences, \n",
        "               'animation': my_animation_sentences,\n",
        "               'grid': my_grid_sentences}"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAT6yk_Ar-aY"
      },
      "source": [
        "pickle.dump(my_commands, open(\"my_commands.p\", \"wb\"))"
      ],
      "execution_count": 39,
      "outputs": []
    }
  ]
}